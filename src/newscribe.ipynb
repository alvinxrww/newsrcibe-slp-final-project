{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-12-18T19:32:34.544124Z","iopub.status.busy":"2024-12-18T19:32:34.543774Z","iopub.status.idle":"2024-12-18T19:34:13.600104Z","shell.execute_reply":"2024-12-18T19:34:13.599232Z","shell.execute_reply.started":"2024-12-18T19:32:34.544090Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mDEPRECATION: git+https://github.com/NVIDIA/NeMo.git@r1.23.0#egg=nemo_toolkit[asr] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n","\u001b[0mName: huggingface-hub\n","Version: 0.26.2\n","Summary: Client library to download and publish models, datasets and other repos on the huggingface.co hub\n","Home-page: https://github.com/huggingface/huggingface_hub\n","Author: Hugging Face, Inc.\n","Author-email: julien@huggingface.co\n","License: Apache\n","Location: /opt/conda/lib/python3.10/site-packages\n","Requires: filelock, fsspec, packaging, pyyaml, requests, tqdm, typing-extensions\n","Required-by: accelerate, datasets, nemo_toolkit, timm, tokenizers, transformers\n","Collecting huggingface-hub==0.23.2\n","  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.23.2) (3.15.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.23.2) (2024.6.0)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.23.2) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.23.2) (6.0.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.23.2) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.23.2) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.23.2) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub==0.23.2) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2) (2024.6.2)\n","Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: huggingface-hub\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.26.2\n","    Uninstalling huggingface-hub-0.26.2:\n","      Successfully uninstalled huggingface-hub-0.26.2\n","Successfully installed huggingface-hub-0.23.2\n"]}],"source":["!pip -q install git+https://github.com/NVIDIA/NeMo.git@r1.23.0#egg=nemo_toolkit[asr]\n","!pip show huggingface-hub\n","!pip install huggingface-hub==0.23.2"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-12-18T19:34:13.603143Z","iopub.status.busy":"2024-12-18T19:34:13.602372Z","iopub.status.idle":"2024-12-18T19:34:27.656582Z","shell.execute_reply":"2024-12-18T19:34:27.655389Z","shell.execute_reply.started":"2024-12-18T19:34:13.603097Z"},"trusted":true},"outputs":[],"source":["!pip install -q pyannote.audio"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-12-18T19:34:27.658223Z","iopub.status.busy":"2024-12-18T19:34:27.657913Z","iopub.status.idle":"2024-12-18T19:34:37.866426Z","shell.execute_reply":"2024-12-18T19:34:37.865274Z","shell.execute_reply.started":"2024-12-18T19:34:27.658191Z"},"trusted":true},"outputs":[],"source":["!pip install -q moviepy"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-12-18T19:34:37.868598Z","iopub.status.busy":"2024-12-18T19:34:37.868318Z","iopub.status.idle":"2024-12-18T19:34:48.557931Z","shell.execute_reply":"2024-12-18T19:34:48.556982Z","shell.execute_reply.started":"2024-12-18T19:34:37.868564Z"},"trusted":true},"outputs":[],"source":["!pip install -q silero-vad"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-12-18T18:42:06.143235Z","iopub.status.busy":"2024-12-18T18:42:06.142957Z","iopub.status.idle":"2024-12-18T18:42:21.286392Z","shell.execute_reply":"2024-12-18T18:42:21.285484Z","shell.execute_reply.started":"2024-12-18T18:42:06.143202Z"},"trusted":true},"outputs":[],"source":["from pyannote.audio import Pipeline\n","from nemo.collections.asr.models import EncDecMultiTaskModel\n","import torch\n","import csv\n","from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n","from pydub import AudioSegment\n","import tempfile\n","\n","# Function 1: Audio to Diarization\n","def audio_to_diarization(audio_path, diarization_file):\n","    pipeline = Pipeline.from_pretrained(\n","        \"pyannote/speaker-diarization-3.1\",\n","        use_auth_token=\"YOUR_HUGGINGFACE_ACCESS_TOKEN\")\n","    pipeline.to(torch.device(\"cuda\"))\n","    diarization_result = []\n","    diarization = pipeline(audio_path)\n","\n","    for turn, _, speaker in diarization.itertracks(yield_label=True):\n","        diarization_result.append({\n","            'start': turn.start, 'end': turn.end, 'speaker': f\"Speaker {speaker}\"})\n","\n","    with open(diarization_file, 'w', newline='') as f:\n","        writer = csv.DictWriter(f, fieldnames=['start', 'end', 'speaker'])\n","        writer.writeheader()\n","        writer.writerows(diarization_result)\n","\n","    return diarization_result\n","\n","# Function 2: Load Diarization\n","def load_diarization(diarization_file):\n","    with open(diarization_file, 'r') as f:\n","        reader = csv.DictReader(f)\n","        return [row for row in reader]\n","\n","# Function to cut audio into segments based on start and end\n","def cut_audio(audio, start_ms, end_ms):\n","    return audio[start_ms:end_ms]\n","\n","# Function to transcribe audio segments\n","def transcribe_segments(audio_path, diarization_result):\n","    model = load_silero_vad()\n","    \n","    canary_model = EncDecMultiTaskModel.from_pretrained('nvidia/canary-1b')\n","    decode_cfg = canary_model.cfg.decoding\n","    decode_cfg.beam.beam_size = 1\n","    canary_model.change_decoding_strategy(decode_cfg)\n","\n","    audio = AudioSegment.from_mp3(audio_path)\n","\n","    transcriptions = []\n","    for segment in diarization_result:\n","        start, end, speaker = float(segment['start']), float(segment['end']), segment['speaker']\n","        \n","        start_ms = int(start * 1000)\n","        end_ms = int(end * 1000)\n","\n","        cut_audio_segment = cut_audio(audio, start_ms, end_ms)\n","        \n","        with tempfile.NamedTemporaryFile(delete=False) as temp_audio_file:\n","            temp_audio_path = temp_audio_file.name + \".wav\"\n","            cut_audio_segment.export(temp_audio_path, format=\"wav\")\n","\n","        wav = read_audio(temp_audio_path)\n","        speech_timestamps = get_speech_timestamps(\n","            wav,\n","            model,\n","            # min_speech_duration_ms=4000,\n","            max_speech_duration_s=30,\n","            return_seconds=True  # Return speech timestamps in seconds\n","        )\n","        print(speech_timestamps)\n","        \n","        for timestamp in speech_timestamps:\n","            segment_start, segment_end = timestamp['start'], timestamp['end']\n","            segment_start_ms = int(segment_start * 1000)\n","            segment_end_ms = int(segment_end * 1000)\n","\n","            audio_chunk = cut_audio(cut_audio_segment, segment_start_ms, segment_end_ms)\n","            \n","            with tempfile.NamedTemporaryFile(delete=False) as chunk_audio_file:\n","                chunk_audio_path = chunk_audio_file.name + \".wav\"\n","                audio_chunk.export(chunk_audio_path, format=\"wav\")\n","            \n","            # Transcribe the chunk using the ASR model\n","            # predicted_text = canary_model.transcribe(paths2audio_files=[chunk_audio_path], batch_size=1)\n","            # predicted_text = canary_model.transcribe(\n","            #     paths2audio_files=[chunk_audio_path],\n","            #     batch_size=16,  # batch size to run the inference with\n","            # )\n","            new_data = {\n","                'start': start + segment_start,\n","                'end': start + segment_end,\n","                'speaker': speaker,\n","                'text': \"dummy\"\n","            }\n","            print(new_data)\n","            # Append the transcription to the list\n","            transcriptions.append(new_data)\n","            \n","    return transcriptions"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-12-18T18:34:21.910027Z","iopub.status.busy":"2024-12-18T18:34:21.909667Z","iopub.status.idle":"2024-12-18T18:34:21.937085Z","shell.execute_reply":"2024-12-18T18:34:21.935744Z","shell.execute_reply.started":"2024-12-18T18:34:21.909990Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'diars' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdiars\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'diars' is not defined"]}],"source":["print(diars)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-12-18T18:42:21.288463Z","iopub.status.busy":"2024-12-18T18:42:21.287738Z","iopub.status.idle":"2024-12-18T18:44:28.616536Z","shell.execute_reply":"2024-12-18T18:44:28.615695Z","shell.execute_reply.started":"2024-12-18T18:42:21.288421Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ca165fb634d487386f5f019d9ed3916","version_major":2,"version_minor":0},"text/plain":["canary-1b.nemo:   0%|          | 0.00/4.07G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[NeMo I 2024-12-18 18:44:03 mixins:196] _setup_tokenizer: detected an aggregate tokenizer\n","[NeMo I 2024-12-18 18:44:03 mixins:330] Tokenizer SentencePieceTokenizer initialized with 32 tokens\n","[NeMo I 2024-12-18 18:44:03 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n","[NeMo I 2024-12-18 18:44:03 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n","[NeMo I 2024-12-18 18:44:03 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n","[NeMo I 2024-12-18 18:44:03 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n","[NeMo I 2024-12-18 18:44:03 aggregate_tokenizer:72] Aggregate vocab size: 4128\n"]},{"name":"stderr","output_type":"stream","text":["[NeMo W 2024-12-18 18:44:03 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n","    Train config : \n","    tarred_audio_filepaths: null\n","    manifest_filepath: null\n","    sample_rate: 16000\n","    shuffle: true\n","    batch_size: null\n","    num_workers: 8\n","    use_lhotse: true\n","    max_duration: 40\n","    pin_memory: true\n","    use_bucketing: false\n","    bucket_duration_bins: null\n","    num_buckets: 1\n","    text_field: answer\n","    lang_field: target_lang\n","    batch_duration: 360\n","    quadratic_duration: 15\n","    bucket_buffer_size: 20000\n","    shuffle_buffer_size: 10000\n","    \n","[NeMo W 2024-12-18 18:44:03 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n","    Validation config : \n","    manifest_filepath: null\n","    sample_rate: 16000\n","    batch_size: 8\n","    shuffle: false\n","    num_workers: 0\n","    pin_memory: true\n","    tarred_audio_filepaths: null\n","    use_lhotse: true\n","    text_field: answer\n","    lang_field: target_lang\n","    use_bucketing: false\n","    \n","[NeMo W 2024-12-18 18:44:03 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n","    Test config : \n","    manifest_filepath: null\n","    sample_rate: 16000\n","    batch_size: 32\n","    shuffle: false\n","    num_workers: 0\n","    pin_memory: true\n","    tarred_audio_filepaths: null\n","    use_lhotse: true\n","    text_field: answer\n","    lang_field: target_lang\n","    use_bucketing: false\n","    \n"]},{"name":"stdout","output_type":"stream","text":["[NeMo I 2024-12-18 18:44:03 features:289] PADDING: 0\n"]},{"name":"stderr","output_type":"stream","text":["[NeMo W 2024-12-18 18:44:15 nemo_logging:349] /opt/conda/lib/python3.10/site-packages/nemo/core/connectors/save_restore_connector.py:571: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","      return torch.load(model_weights, map_location='cpu')\n","    \n"]},{"name":"stdout","output_type":"stream","text":["[NeMo I 2024-12-18 18:44:19 save_restore_connector:249] Model EncDecMultiTaskModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--canary-1b/snapshots/dd32c0c709e2bfc79f583e16b9df4b3a160f7e86/canary-1b.nemo.\n","[NeMo I 2024-12-18 18:44:19 aed_multitask_models:214] Changed decoding strategy to \n","    strategy: beam\n","    compute_hypothesis_token_set: false\n","    preserve_alignments: null\n","    compute_langs: false\n","    beam:\n","      beam_size: 1\n","      search_type: default\n","      len_pen: 1.0\n","      max_generation_delta: 20\n","      return_best_hypothesis: true\n","      preserve_alignments: false\n","    temperature: 1.0\n","    \n","[{'start': 0.1, 'end': 14.2}, {'start': 14.4, 'end': 23.0}, {'start': 23.2, 'end': 24.9}, {'start': 25.3, 'end': 32.0}, {'start': 32.2, 'end': 37.2}, {'start': 37.4, 'end': 42.0}, {'start': 42.1, 'end': 44.5}, {'start': 44.8, 'end': 49.2}, {'start': 49.3, 'end': 52.4}, {'start': 52.7, 'end': 61.7}, {'start': 61.8, 'end': 64.4}, {'start': 64.7, 'end': 66.4}, {'start': 66.6, 'end': 71.4}, {'start': 71.5, 'end': 74.3}]\n","{'start': 0.13096875, 'end': 14.230968749999999, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 14.43096875, 'end': 23.03096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 23.23096875, 'end': 24.930968749999998, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 25.33096875, 'end': 32.03096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 32.23096875, 'end': 37.23096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 37.43096875, 'end': 42.03096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 42.13096875, 'end': 44.53096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 44.83096875, 'end': 49.23096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 49.33096875, 'end': 52.43096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 52.73096875, 'end': 61.73096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 61.83096875, 'end': 64.43096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 64.73096875, 'end': 66.43096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 66.63096875, 'end': 71.43096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 71.53096875, 'end': 74.33096875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","[{'start': 0.0, 'end': 2.0}, {'start': 2.2, 'end': 17.9}, {'start': 18.1, 'end': 19.6}, {'start': 19.7, 'end': 30.2}, {'start': 30.6, 'end': 33.3}, {'start': 33.6, 'end': 36.6}, {'start': 36.8, 'end': 49.8}]\n","{'start': 74.29784375, 'end': 76.29784375, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","{'start': 76.49784375, 'end': 92.19784375, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","{'start': 92.39784374999999, 'end': 93.89784374999999, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","{'start': 93.99784375, 'end': 104.49784375, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","{'start': 104.89784374999999, 'end': 107.59784375, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","{'start': 107.89784374999999, 'end': 110.89784374999999, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","{'start': 111.09784375, 'end': 124.09784375, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","[{'start': 0.1, 'end': 13.2}, {'start': 13.4, 'end': 17.1}]\n","{'start': 124.19596875, 'end': 137.29596875000001, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 137.49596875, 'end': 141.19596875000002, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","[{'start': 0.1, 'end': 30.1}, {'start': 30.1, 'end': 46.2}]\n","{'start': 141.27346875, 'end': 171.27346875, 'speaker': 'Speaker SPEAKER_03', 'text': 'dummy'}\n","{'start': 171.27346875, 'end': 187.37346875000003, 'speaker': 'Speaker SPEAKER_03', 'text': 'dummy'}\n","[{'start': 0.0, 'end': 13.9}, {'start': 14.1, 'end': 16.7}]\n","{'start': 187.25909375, 'end': 201.15909375, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 201.35909375, 'end': 203.95909375, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","[{'start': 0.1, 'end': 2.7}]\n","{'start': 206.00596875, 'end': 208.60596875, 'speaker': 'Speaker SPEAKER_01', 'text': 'dummy'}\n","[{'start': 0.1, 'end': 1.6}, {'start': 1.8, 'end': 2.7}, {'start': 2.8, 'end': 4.9}, {'start': 5.0, 'end': 7.4}]\n","{'start': 209.02659375000002, 'end': 210.52659375000002, 'speaker': 'Speaker SPEAKER_01', 'text': 'dummy'}\n","{'start': 210.72659375000003, 'end': 211.62659375, 'speaker': 'Speaker SPEAKER_01', 'text': 'dummy'}\n","{'start': 211.72659375000003, 'end': 213.82659375000003, 'speaker': 'Speaker SPEAKER_01', 'text': 'dummy'}\n","{'start': 213.92659375000002, 'end': 216.32659375000003, 'speaker': 'Speaker SPEAKER_01', 'text': 'dummy'}\n","[{'start': 0.1, 'end': 9.5}, {'start': 9.7, 'end': 20.4}]\n","{'start': 217.68346875, 'end': 227.08346875, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 227.28346875, 'end': 237.98346875000001, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","[{'start': 0.0, 'end': 22.6}, {'start': 22.8, 'end': 31.3}, {'start': 31.4, 'end': 37.1}, {'start': 37.2, 'end': 46.5}, {'start': 46.7, 'end': 53.0}, {'start': 53.3, 'end': 55.4}, {'start': 55.6, 'end': 59.7}]\n","{'start': 238.01909375000002, 'end': 260.61909375000005, 'speaker': 'Speaker SPEAKER_02', 'text': 'dummy'}\n","{'start': 260.81909375000004, 'end': 269.31909375000004, 'speaker': 'Speaker SPEAKER_02', 'text': 'dummy'}\n","{'start': 269.41909375, 'end': 275.11909375000005, 'speaker': 'Speaker SPEAKER_02', 'text': 'dummy'}\n","{'start': 275.21909375, 'end': 284.51909375, 'speaker': 'Speaker SPEAKER_02', 'text': 'dummy'}\n","{'start': 284.71909375, 'end': 291.01909375, 'speaker': 'Speaker SPEAKER_02', 'text': 'dummy'}\n","{'start': 291.31909375000004, 'end': 293.41909375, 'speaker': 'Speaker SPEAKER_02', 'text': 'dummy'}\n","{'start': 293.61909375000005, 'end': 297.71909375, 'speaker': 'Speaker SPEAKER_02', 'text': 'dummy'}\n","[{'start': 0.1, 'end': 2.2}, {'start': 2.5, 'end': 5.9}]\n","{'start': 298.14346875000007, 'end': 300.24346875000003, 'speaker': 'Speaker SPEAKER_02', 'text': 'dummy'}\n","{'start': 300.54346875000005, 'end': 303.94346875, 'speaker': 'Speaker SPEAKER_02', 'text': 'dummy'}\n","[{'start': 0.1, 'end': 13.0}, {'start': 13.1, 'end': 16.9}, {'start': 17.0, 'end': 26.1}, {'start': 26.2, 'end': 26.9}, {'start': 27.0, 'end': 37.3}]\n","{'start': 304.08346875000007, 'end': 316.98346875000004, 'speaker': 'Speaker SPEAKER_00', 'text': 'dummy'}\n","{'start': 317.08346875000007, 'end': 320.88346875, 'speaker': 'Speaker SPEAKER_00', 'text': 'dummy'}\n","{'start': 320.98346875000004, 'end': 330.08346875000007, 'speaker': 'Speaker SPEAKER_00', 'text': 'dummy'}\n","{'start': 330.18346875000003, 'end': 330.88346875, 'speaker': 'Speaker SPEAKER_00', 'text': 'dummy'}\n","{'start': 330.98346875000004, 'end': 341.28346875000005, 'speaker': 'Speaker SPEAKER_00', 'text': 'dummy'}\n","[{'start': 0.0, 'end': 11.2}, {'start': 11.3, 'end': 12.5}]\n","{'start': 340.82159375000003, 'end': 352.02159375, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 352.12159375000005, 'end': 353.32159375000003, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","[{'start': 0.2, 'end': 18.1}, {'start': 18.2, 'end': 20.0}]\n","{'start': 353.47534375000004, 'end': 371.37534375000007, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","{'start': 371.47534375000004, 'end': 373.27534375000005, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","[{'start': 0.0, 'end': 2.4}]\n","{'start': 372.31034375, 'end': 374.71034375, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","[{'start': 0.0, 'end': 23.3}, {'start': 23.4, 'end': 33.9}]\n","{'start': 373.79534375000003, 'end': 397.09534375000004, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","{'start': 397.19534375, 'end': 407.69534375, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","[{'start': 0.0, 'end': 29.9}, {'start': 29.9, 'end': 32.2}]\n","{'start': 405.63846875, 'end': 435.53846875, 'speaker': 'Speaker SPEAKER_03', 'text': 'dummy'}\n","{'start': 435.53846875, 'end': 437.83846875, 'speaker': 'Speaker SPEAKER_03', 'text': 'dummy'}\n","[{'start': 0.0, 'end': 4.8}, {'start': 5.0, 'end': 5.9}, {'start': 6.6, 'end': 7.8}]\n","{'start': 437.71784375000004, 'end': 442.51784375000005, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 442.71784375000004, 'end': 443.61784375, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","{'start': 444.31784375000007, 'end': 445.51784375000005, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n","[]\n","[]\n","[]\n","[{'start': 2.8, 'end': 3.4}, {'start': 3.6, 'end': 5.5}]\n","{'start': 444.29784375, 'end': 444.89784375, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","{'start': 445.09784375000004, 'end': 446.99784375, 'speaker': 'Speaker SPEAKER_04', 'text': 'dummy'}\n","[{'start': 0.0, 'end': 1.3}]\n","{'start': 446.49284375, 'end': 447.79284375000003, 'speaker': 'Speaker SPEAKER_05', 'text': 'dummy'}\n"]}],"source":["diarization_file = \"/kaggle/input/anehba/diarization (1).csv\"\n","audio_file = \"/kaggle/input/anehba/berita_bule.mp3\"\n","diars = load_diarization(diarization_file)\n","transcriptions = transcribe_segments(audio_file, diars)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-12-18T18:45:53.289729Z","iopub.status.busy":"2024-12-18T18:45:53.289399Z","iopub.status.idle":"2024-12-18T18:45:53.300388Z","shell.execute_reply":"2024-12-18T18:45:53.299610Z","shell.execute_reply.started":"2024-12-18T18:45:53.289704Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","# List of dictionaries\n","data = transcriptions\n","# Convert to DataFrame\n","df = pd.DataFrame(data)\n","\n","# Save to CSV\n","df.to_csv('output.csv', index=False)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-12-18T19:38:43.114572Z","iopub.status.busy":"2024-12-18T19:38:43.114202Z","iopub.status.idle":"2024-12-18T19:39:03.536382Z","shell.execute_reply":"2024-12-18T19:39:03.535412Z","shell.execute_reply.started":"2024-12-18T19:38:43.114536Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[NeMo I 2024-12-18 19:38:48 mixins:196] _setup_tokenizer: detected an aggregate tokenizer\n","[NeMo I 2024-12-18 19:38:48 mixins:330] Tokenizer SentencePieceTokenizer initialized with 32 tokens\n","[NeMo I 2024-12-18 19:38:48 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n","[NeMo I 2024-12-18 19:38:48 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n","[NeMo I 2024-12-18 19:38:48 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n","[NeMo I 2024-12-18 19:38:48 mixins:330] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n","[NeMo I 2024-12-18 19:38:48 aggregate_tokenizer:72] Aggregate vocab size: 4128\n"]},{"name":"stderr","output_type":"stream","text":["[NeMo W 2024-12-18 19:38:48 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n","    Train config : \n","    tarred_audio_filepaths: null\n","    manifest_filepath: null\n","    sample_rate: 16000\n","    shuffle: true\n","    batch_size: null\n","    num_workers: 8\n","    use_lhotse: true\n","    max_duration: 40\n","    pin_memory: true\n","    use_bucketing: false\n","    bucket_duration_bins: null\n","    num_buckets: 1\n","    text_field: answer\n","    lang_field: target_lang\n","    batch_duration: 360\n","    quadratic_duration: 15\n","    bucket_buffer_size: 20000\n","    shuffle_buffer_size: 10000\n","    \n","[NeMo W 2024-12-18 19:38:48 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n","    Validation config : \n","    manifest_filepath: null\n","    sample_rate: 16000\n","    batch_size: 8\n","    shuffle: false\n","    num_workers: 0\n","    pin_memory: true\n","    tarred_audio_filepaths: null\n","    use_lhotse: true\n","    text_field: answer\n","    lang_field: target_lang\n","    use_bucketing: false\n","    \n","[NeMo W 2024-12-18 19:38:48 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n","    Test config : \n","    manifest_filepath: null\n","    sample_rate: 16000\n","    batch_size: 32\n","    shuffle: false\n","    num_workers: 0\n","    pin_memory: true\n","    tarred_audio_filepaths: null\n","    use_lhotse: true\n","    text_field: answer\n","    lang_field: target_lang\n","    use_bucketing: false\n","    \n"]},{"name":"stdout","output_type":"stream","text":["[NeMo I 2024-12-18 19:38:48 features:289] PADDING: 0\n","[NeMo I 2024-12-18 19:39:03 save_restore_connector:249] Model EncDecMultiTaskModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--canary-1b/snapshots/dd32c0c709e2bfc79f583e16b9df4b3a160f7e86/canary-1b.nemo.\n"]}],"source":["from nemo.collections.asr.models import EncDecMultiTaskModel\n","from pydub import AudioSegment\n","\n","model = EncDecMultiTaskModel.from_pretrained('nvidia/canary-1b')\n","\n","def transcribe_audio(audio_file_path):\n","    decode_cfg = model.cfg.decoding\n","    decode_cfg.beam.beam_size = 1\n","    model.change_decoding_strategy(decode_cfg)\n","    \n","    # Transcribe audio\n","    predicted_text = model.transcribe(\n","        paths2audio_files=[audio_file_path],\n","        batch_size=16,\n","    )\n","    \n","    return predicted_text\n","\n","def crop_audio(input_file, start_time, end_time, output_file):\n","    audio = AudioSegment.from_file(input_file)\n","    cropped_audio = audio[start_time * 1000:end_time * 1000]\n","    cropped_audio.export(output_file, format=\"mp3\")\n","    return output_file"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-12-18T19:32:12.592400Z","iopub.status.busy":"2024-12-18T19:32:12.592149Z","iopub.status.idle":"2024-12-18T19:32:13.608424Z","shell.execute_reply":"2024-12-18T19:32:13.606965Z","shell.execute_reply.started":"2024-12-18T19:32:12.592373Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'crop_audio' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m cropped_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/cropped_audio_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Crop audio\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m cropped_audio_file \u001b[38;5;241m=\u001b[39m \u001b[43mcrop_audio\u001b[49m(input_file, start_time, end_time, cropped_file)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Transcribe audio\u001b[39;00m\n\u001b[1;32m     17\u001b[0m result \u001b[38;5;241m=\u001b[39m transcribe_audio(cropped_audio_file)\n","\u001b[0;31mNameError\u001b[0m: name 'crop_audio' is not defined"]}],"source":["import pandas as pd\n","\n","output_file = pd.read_csv(\"/kaggle/working/output.csv\")\n","\n","for index, row in output_file.iterrows():\n","    start_time = row['start']\n","    end_time = row['end']\n","    input_file = \"/kaggle/input/anehba/berita_bule.mp3\"\n","    cropped_file = f\"/kaggle/working/cropped_audio_{index}.wav\"\n","    \n","    cropped_audio_file = crop_audio(input_file, start_time, end_time, cropped_file)\n","    \n","    result = transcribe_audio(cropped_audio_file)\n","    print(result)\n","    \n","    output_file.at[index, 'text'] = result[0] if result else \"Transcription failed\"\n","\n","output_file.to_csv(\"/kaggle/working/output_updated.csv\", index=False)\n","print(\"Transcription completed and CSV updated.\")"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-12-18T20:30:31.798376Z","iopub.status.busy":"2024-12-18T20:30:31.797938Z","iopub.status.idle":"2024-12-18T20:30:33.603419Z","shell.execute_reply":"2024-12-18T20:30:33.602462Z","shell.execute_reply.started":"2024-12-18T20:30:31.798325Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[NeMo I 2024-12-18 20:30:33 aed_multitask_models:214] Changed decoding strategy to \n","    strategy: beam\n","    compute_hypothesis_token_set: false\n","    preserve_alignments: null\n","    compute_langs: false\n","    beam:\n","      beam_size: 1\n","      search_type: default\n","      len_pen: 1.0\n","      max_generation_delta: 20\n","      return_best_hypothesis: true\n","      preserve_alignments: false\n","    temperature: 1.0\n","    \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"842342e072574f60844351f874cc78b2","version_major":2,"version_minor":0},"text/plain":["Transcribing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[NeMo W 2024-12-18 20:30:33 nemo_logging:349] /opt/conda/lib/python3.10/site-packages/nemo/collections/asr/parts/preprocessing/features.py:417: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","      with torch.cuda.amp.autocast(enabled=False):\n","    \n"]},{"name":"stdout","output_type":"stream","text":["['Tiple A.']\n"]}],"source":["from pydub import AudioSegment\n","\n","def crop_audio(input_file, start_time, end_time, output_file):\n","    audio = AudioSegment.from_file(input_file)\n","    cropped_audio = audio[start_time * 1000:end_time * 1000]\n","    cropped_audio.export(output_file, format=\"mp3\")\n","    return output_file\n","\n","input_file = \"/kaggle/input/anehba/berita_bule.mp3\"\n","start_time = 444.29784375 # Start tim141.27346875e in seconds\n","end_time = 444.89784375 # End time in seconds\n","output_file = \"/kaggle/working/cropped_audio.mp3\"\n","\n","cropped_audio_file = crop_audio(input_file, start_time, end_time, output_file)\n","result = transcribe_audio(cropped_audio_file)\n","print(result)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":6170042,"sourceId":10238848,"sourceType":"datasetVersion"}],"dockerImageVersionId":30805,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
